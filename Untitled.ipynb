{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from graphsage.model import load_cora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import defaultdict\n",
    "\n",
    "from graphsage.encoders import Encoder\n",
    "from graphsage.aggregators import MeanAggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaesonyoon/graphsage-simple/graphsage/encoders.py:31: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(self.weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(61.6892) tensor(0.4678) tensor(0.4621)\n",
      "20 tensor(60.9267) tensor(0.4447) tensor(0.4442)\n",
      "40 tensor(58.5197) tensor(0.4248) tensor(0.4293)\n",
      "60 tensor(55.5350) tensor(0.4109) tensor(0.4200)\n",
      "80 tensor(53.4406) tensor(0.4003) tensor(0.4097)\n",
      "100 tensor(52.1675) tensor(0.3912) tensor(0.4030)\n",
      "120 tensor(49.2007) tensor(0.3846) tensor(0.3976)\n",
      "140 tensor(51.0180) tensor(0.3782) tensor(0.3925)\n",
      "160 tensor(49.9771) tensor(0.3733) tensor(0.3893)\n",
      "180 tensor(51.3473) tensor(0.3690) tensor(0.3854)\n",
      "200 tensor(47.6207) tensor(0.3654) tensor(0.3831)\n",
      "220 tensor(46.9435) tensor(0.3616) tensor(0.3784)\n",
      "240 tensor(46.3222) tensor(0.3586) tensor(0.3762)\n",
      "260 tensor(48.4123) tensor(0.3570) tensor(0.3736)\n",
      "280 tensor(48.5557) tensor(0.3548) tensor(0.3720)\n",
      "300 tensor(46.3415) tensor(0.3528) tensor(0.3706)\n",
      "320 tensor(47.5181) tensor(0.3513) tensor(0.3673)\n",
      "340 tensor(47.1403) tensor(0.3499) tensor(0.3673)\n",
      "360 tensor(45.5448) tensor(0.3483) tensor(0.3653)\n",
      "380 tensor(45.7761) tensor(0.3473) tensor(0.3643)\n",
      "400 tensor(47.5430) tensor(0.3457) tensor(0.3628)\n",
      "420 tensor(46.4133) tensor(0.3441) tensor(0.3621)\n",
      "440 tensor(45.9016) tensor(0.3430) tensor(0.3602)\n",
      "460 tensor(46.2398) tensor(0.3419) tensor(0.3635)\n",
      "480 tensor(47.8409) tensor(0.3408) tensor(0.3628)\n",
      "500 tensor(46.2318) tensor(0.3402) tensor(0.3613)\n",
      "520 tensor(45.8400) tensor(0.3389) tensor(0.3606)\n",
      "540 tensor(46.8210) tensor(0.3383) tensor(0.3589)\n",
      "560 tensor(47.2527) tensor(0.3373) tensor(0.3577)\n",
      "580 tensor(48.2172) tensor(0.3368) tensor(0.3588)\n",
      "600 tensor(45.6082) tensor(0.3363) tensor(0.3572)\n",
      "620 tensor(45.3332) tensor(0.3361) tensor(0.3568)\n",
      "640 tensor(46.5340) tensor(0.3360) tensor(0.3560)\n",
      "660 tensor(44.3481) tensor(0.3360) tensor(0.3555)\n",
      "680 tensor(47.1580) tensor(0.3358) tensor(0.3557)\n",
      "700 tensor(45.3150) tensor(0.3352) tensor(0.3551)\n",
      "720 tensor(45.2450) tensor(0.3349) tensor(0.3540)\n",
      "740 tensor(46.6286) tensor(0.3346) tensor(0.3539)\n",
      "760 tensor(45.5021) tensor(0.3344) tensor(0.3535)\n",
      "780 tensor(46.3103) tensor(0.3342) tensor(0.3528)\n",
      "800 tensor(46.1726) tensor(0.3339) tensor(0.3526)\n",
      "820 tensor(45.9164) tensor(0.3335) tensor(0.3518)\n",
      "840 tensor(45.0906) tensor(0.3334) tensor(0.3523)\n",
      "860 tensor(45.3089) tensor(0.3333) tensor(0.3522)\n",
      "880 tensor(47.3635) tensor(0.3332) tensor(0.3522)\n",
      "900 tensor(46.8212) tensor(0.3325) tensor(0.3515)\n",
      "920 tensor(45.5034) tensor(0.3323) tensor(0.3509)\n",
      "940 tensor(45.0167) tensor(0.3322) tensor(0.3507)\n",
      "960 tensor(46.3034) tensor(0.3321) tensor(0.3509)\n",
      "980 tensor(45.0850) tensor(0.3319) tensor(0.3504)\n"
     ]
    }
   ],
   "source": [
    "class SupervisedGraphSage(nn.Module):\n",
    "\n",
    "    def __init__(self, enc, degree_list):\n",
    "        super(SupervisedGraphSage, self).__init__()\n",
    "        self.enc = enc\n",
    "        self.xent = nn.BCELoss()\n",
    "        wt = np.power(degree_list, 0.75)\n",
    "        wt = wt / wt.sum()\n",
    "        self.weights = torch.FloatTensor(wt)\n",
    "        \n",
    "    def negative_sample(self, number_of_neg_sample):\n",
    "        return torch.multinomial(self.weights, number_of_neg_sample, \n",
    "                                 replacement=True)\n",
    "\n",
    "    def forward(self, nodes):\n",
    "        embeds = self.enc(nodes)\n",
    "        return embeds\n",
    "    \n",
    "    def affinity(self, input_1, input_2):\n",
    "        output_1 = torch.nn.functional.normalize(self.forward(input_1))\n",
    "        output_2 = torch.nn.functional.normalize(self.forward(input_2))\n",
    "        aff = torch.sum((output_1 * output_2), dim=1)\n",
    "        \n",
    "        return output_1, aff\n",
    "    \n",
    "    def neg_affinity(self, output_1, neg_samples):\n",
    "        neg_output = torch.nn.functional.normalize(self.forward(neg_samples))\n",
    "        neg_aff = torch.mm(output_1.t(),neg_output)\n",
    "        \n",
    "        return neg_aff\n",
    "\n",
    "    def loss(self, edges, neg_samples):\n",
    "        input_1 = [edge[0] for edge in edges]\n",
    "        input_2 = [edge[1] for edge in edges]\n",
    "        \n",
    "        output_1, aff = self.affinity(input_1, input_2)\n",
    "        neg_aff = self.neg_affinity(output_1, neg_samples)\n",
    "        \n",
    "        total_loss = 0\n",
    "        total_loss += - torch.sum(torch.log(torch.sigmoid(aff)))\n",
    "        total_loss += - len(neg_samples) * torch.sum((torch.log(torch.sigmoid(-neg_aff))))\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def compare_loss(self, edges):\n",
    "        input_1 = [edge[0] for edge in edges]\n",
    "        input_2 = [edge[1] for edge in edges]\n",
    "        \n",
    "        _, aff = self.affinity(input_1, input_2)\n",
    "        total_loss = - torch.mean(torch.log(torch.sigmoid(aff)))\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "num_nodes = 2708\n",
    "feat_data, labels, adj_lists = load_cora()\n",
    "features = nn.Embedding(2708, 1433)\n",
    "features.weight = nn.Parameter(torch.FloatTensor(feat_data), requires_grad=False)\n",
    "agg1 = MeanAggregator(features, cuda=True)\n",
    "enc1 = Encoder(features, 1433, 128, adj_lists, agg1, gcn=True, cuda=False)\n",
    "agg2 = MeanAggregator(lambda nodes : enc1(nodes).t(), cuda=False)\n",
    "enc2 = Encoder(lambda nodes : enc1(nodes).t(), enc1.embed_dim, 128, adj_lists, agg2,\n",
    "            base_model=enc1, gcn=True, cuda=False)\n",
    "enc1.num_samples = 5\n",
    "enc2.num_samples = 5\n",
    "rand_indices = np.random.permutation(num_nodes)\n",
    "test = rand_indices[:1000]\n",
    "val = rand_indices[1000:1500]\n",
    "train = list(rand_indices[1500:])\n",
    "train_degree_list = [len(adj_lists[node]) for node in train]\n",
    "train_edges = [(row, node) for row in train for node in adj_lists[row] if node in train]\n",
    "val_edges = [(row, node) for row in val for node in adj_lists[row] if node not in test]\n",
    "graphsage = SupervisedGraphSage(enc2, train_degree_list)\n",
    "optimizer = torch.optim.Adam(filter(lambda p : p.requires_grad, graphsage.parameters()), lr=0.0001)\n",
    "batch_size = 30\n",
    "number_of_neg_sample = 20\n",
    "times = []\n",
    "for batch in range(1000):\n",
    "    batch_edges = train_edges[:batch_size]\n",
    "    neg_samples = graphsage.negative_sample(number_of_neg_sample)\n",
    "    random.shuffle(train_edges)\n",
    "    start_time = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    loss = graphsage.loss(batch_edges, neg_samples)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    end_time = time.time()\n",
    "    times.append(end_time-start_time)\n",
    "    if batch % 20 == 0:\n",
    "        loss_train =  graphsage.compare_loss(train_edges)\n",
    "        loss_val = graphsage.compare_loss(val_edges)\n",
    "        print(batch, loss.data, loss_train.data, loss_val.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val = rand_indices[1000:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_edges = [(row, node) for row in val for node in adj_lists[row] if node not in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
